{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project SVD comparison.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXmKtetHWuGZ",
        "outputId": "4bb478c7-e7c1-44c0-b40d-d24a6e9320a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-sQqRw_WyLy"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/ColabNotebooks/10605/project')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWwYcISmGyV0"
      },
      "source": [
        "\n",
        "#imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioD7uhTD_lzq"
      },
      "source": [
        "#mean and std of cifar100 dataset\n",
        "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "WARM = False # typically used in new training\n",
        "\n",
        "DATE_FORMAT = '%A_%d_%B_%Y_%Hh_%Mm_%Ss'\n",
        "#time of we run the script\n",
        "TIME_NOW = datetime.datetime.now().strftime(DATE_FORMAT)\n",
        "#data settings\n",
        "subset = False #for local running\n",
        "k = 10 #number of samples needed to each class in validation set, because we need to split train and validation\n",
        "\n",
        "#model settings\n",
        "USE_TENSORBOARD = False\n",
        "if USE_TENSORBOARD:\n",
        "    foo = SummaryWriter()\n",
        "use_gpu = True\n",
        "\n",
        "#lr scheduler\n",
        "BASE_LR = 0.001\n",
        "EPOCH_DECAY = 4\n",
        "DECAY_WEIGHT = 0.5\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "if use_gpu and torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLsDydTz_zPV"
      },
      "source": [
        "#read files\n",
        "def unpickle(file):\n",
        "    \n",
        "    with open(file, 'rb') as fo:\n",
        "        dictionary = pickle.load(fo, encoding='bytes')\n",
        "    return dictionary"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-LF6Hap_3eU"
      },
      "source": [
        "def compute_mean_std(cifar100_dataset):\n",
        "    \"\"\"compute the mean and std of cifar100 dataset\n",
        "    Args:\n",
        "        cifar100_training_dataset or cifar100_test_dataset\n",
        "        witch derived from class torch.utils.data\n",
        "\n",
        "    Returns:\n",
        "        a tuple contains mean, std value of entire dataset\n",
        "    \"\"\"\n",
        "\n",
        "    data_r = numpy.dstack([cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))])\n",
        "    data_g = numpy.dstack([cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))])\n",
        "    data_b = numpy.dstack([cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))])\n",
        "    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n",
        "    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n",
        "\n",
        "    return mean, std"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz6Rkrf__3gw"
      },
      "source": [
        "#data processing\n",
        "def reshape_images(data_dict):\n",
        "    reshaped = data_dict.numpy().reshape(len(data_dict), 1024, 3, order = 'F').reshape(len(data_dict), 32,32,3)\n",
        "    reshaped_processed = torch.from_numpy(reshaped).float().permute(0, 3, 1, 2)\n",
        "    return reshaped_processed"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2vPrB9InvV1"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bue64qDGnyF2"
      },
      "source": [
        "## Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWcyPIGy_3jP"
      },
      "source": [
        "def get_training_val_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
        "    \"\"\" return training dataloader\n",
        "    Args:\n",
        "        mean: mean of cifar100 training dataset\n",
        "        std: std of cifar100 training dataset\n",
        "        path: path to cifar100 training python dataset\n",
        "        batch_size: dataloader batchsize\n",
        "        num_workers: dataloader num_works\n",
        "        shuffle: whether to shuffle\n",
        "    Returns: train_data_loader:torch dataloader object\n",
        "    \"\"\"\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        #transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    cifar100_training = torchvision.datasets.CIFAR100(root='', train=True, download=True, transform=transform_train)\n",
        "    \n",
        "    try:\n",
        "        random_index = pickle.load(open(\"random_index.pkl\", 'rb'))\n",
        "    except:\n",
        "        random_index = np.random.permutation([i for i in range(50000)])\n",
        "        pickle.dump(random_index, open(\"random_index.pkl\", 'wb'))\n",
        "    \n",
        "    train_index = random_index[:45000]\n",
        "    validation_index = random_index[45000:]\n",
        "    train_dataset = torch.utils.data.Subset(cifar100_training, train_index)\n",
        "    validation_dataset = torch.utils.data.Subset(cifar100_training, validation_index)\n",
        "    \n",
        "    cifar100_training_loader = DataLoader(\n",
        "        train_dataset, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "    \n",
        "    cifar100_validation_loader = DataLoader(\n",
        "        validation_dataset, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return cifar100_training_loader, cifar100_validation_loader"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dtl4tgr_zUS"
      },
      "source": [
        "def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
        "    \"\"\" return training dataloader\n",
        "    Args:\n",
        "        mean: mean of cifar100 training dataset\n",
        "        std: std of cifar100 training dataset\n",
        "        path: path to cifar100 training python dataset\n",
        "        batch_size: dataloader batchsize\n",
        "        num_workers: dataloader num_works\n",
        "        shuffle: whether to shuffle\n",
        "    Returns: train_data_loader:torch dataloader object\n",
        "    \"\"\"\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        #transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    cifar100_training = torchvision.datasets.CIFAR100(root='cifar-100-python', train=True, download=True, transform=transform_train)\n",
        "    \n",
        "    cifar100_training_loader = DataLoader(\n",
        "        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return cifar100_training_loader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flXIjXIKADyv"
      },
      "source": [
        "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
        "    \"\"\" return training dataloader\n",
        "    Args:\n",
        "        mean: mean of cifar100 test dataset\n",
        "        std: std of cifar100 test dataset\n",
        "        path: path to cifar100 test python dataset\n",
        "        batch_size: dataloader batchsize\n",
        "        num_workers: dataloader num_works\n",
        "        shuffle: whether to shuffle\n",
        "    Returns: cifar100_test_loader:torch dataloader object\n",
        "    \"\"\"\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    cifar100_test = torchvision.datasets.CIFAR100(root='cifar-100-python', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    cifar100_test_loader = DataLoader(\n",
        "        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return cifar100_test_loader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFQWFwte-gTf"
      },
      "source": [
        "## train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Kh6ULFwhC9"
      },
      "source": [
        "def train(model, epoch, train_dataloader, optimizer, loss_function, callbacks = None):\n",
        "\n",
        "    start = time.time()\n",
        "    model.to(DEVICE)\n",
        "    model.train()\n",
        "    # keep track of the zero mask\n",
        "    if callbacks != None:\n",
        "        callbacks.get_zeros_mask(model)\n",
        "    \n",
        "    for batch_index, (images, labels) in enumerate(train_dataloader):\n",
        "        '''\n",
        "        if epoch <= WARM:\n",
        "            warmup_scheduler.step()\n",
        "        '''\n",
        "            \n",
        "        if use_gpu:\n",
        "            labels = labels.to(DEVICE)\n",
        "            images = images.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if callbacks != None:\n",
        "            callbacks.apply_zeros_mask(model)\n",
        "            \n",
        "        n_iter = (epoch - 1) * len(train_dataloader) + batch_index + 1\n",
        "\n",
        "        last_layer = list(model.children())[-1]\n",
        "        for name, para in last_layer.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
        "            if 'bias' in name:\n",
        "                writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n",
        "\n",
        "        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
        "            loss.item(),\n",
        "            optimizer.param_groups[0]['lr'],\n",
        "            epoch=epoch,\n",
        "            trained_samples=batch_index * BATCH_SIZE + len(images),\n",
        "            total_samples=len(train_dataloader.dataset)\n",
        "        ))\n",
        "\n",
        "        #update training loss for each iteration\n",
        "        writer.add_scalar('Train/loss', loss.item(), n_iter)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        layer, attr = os.path.splitext(name)\n",
        "        attr = attr[1:]\n",
        "        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n",
        "\n",
        "    finish = time.time()\n",
        "\n",
        "    print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLfgZU2FAD37"
      },
      "source": [
        "def evaluate_model(model, val_dataloader):\n",
        "    # for validation set or testing set\n",
        "    start = time.time()\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    \n",
        "    total_preds = 0\n",
        "    total_corrects = 0\n",
        "    \n",
        "    for batch_index, (images, labels) in enumerate(val_dataloader):\n",
        "        if use_gpu:\n",
        "            # labels = labels.to(DEVICE)\n",
        "            images = images.to(DEVICE)\n",
        "            \n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        \n",
        "        \n",
        "        total_preds += len(labels)\n",
        "        total_corrects += np.sum(preds.cpu().numpy() == labels.numpy())\n",
        "    \n",
        "    # print(\"Accuracy is {:.5f}\".format(total_corrects/total_preds))\n",
        "    \n",
        "    return total_corrects/total_preds"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GBTr6am_zXZ",
        "outputId": "1fabf7ab-cee0-46c7-e56e-eeb44c14d116"
      },
      "source": [
        "'''\n",
        "cifar100_training_loader, cifar100_validation_loader = get_training_dataloader(\n",
        "    CIFAR100_TRAIN_MEAN,\n",
        "    CIFAR100_TRAIN_STD,\n",
        "    num_workers = 4,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True\n",
        ")\n",
        "'''\n",
        "cifar100_training_loader = get_training_dataloader(\n",
        "    CIFAR100_TRAIN_MEAN,\n",
        "    CIFAR100_TRAIN_STD,\n",
        "    num_workers = 4,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "\n",
        "cifar100_test_loader = get_test_dataloader(\n",
        "    CIFAR100_TRAIN_MEAN,\n",
        "    CIFAR100_TRAIN_STD,\n",
        "    num_workers = 4,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul1aziRYp1ef"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJKgiPK8p4Ay"
      },
      "source": [
        "## model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hUxooMzp1hW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "cfg = {\n",
        "    'A' : [64,     'M', 128,      'M', 256, 256,           'M', 512, 512,           'M', 512, 512,           'M'],\n",
        "    'B' : [64, 64, 'M', 128, 128, 'M', 256, 256,           'M', 512, 512,           'M', 512, 512,           'M'],\n",
        "    'D' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256,      'M', 512, 512, 512,      'M', 512, 512, 512,      'M'],\n",
        "    'E' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_class=100):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.features(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.classifier(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "\n",
        "    input_channel = 3\n",
        "    for l in cfg:\n",
        "        if l == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            continue\n",
        "\n",
        "        layers += [nn.Conv2d(input_channel, l, kernel_size=3, padding=1)]\n",
        "\n",
        "        if batch_norm:\n",
        "            layers += [nn.BatchNorm2d(l)]\n",
        "\n",
        "        layers += [nn.ReLU(inplace=True)]\n",
        "        input_channel = l\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def vgg11_bn():\n",
        "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
        "\n",
        "def vgg13_bn():\n",
        "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
        "\n",
        "def vgg16_bn():\n",
        "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
        "\n",
        "def vgg19_bn():\n",
        "    return VGG(make_layers(cfg['E'], batch_norm=True))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy1J4zXtFHCv"
      },
      "source": [
        "def load_vgg(path):\n",
        "    model = vgg16_bn()\n",
        "    weights = torch.load(path)\n",
        "    model.load_state_dict(weights)\n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuy9USnIFHFf"
      },
      "source": [
        "model = load_vgg(\"vgg16-197-best.pth\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JXivkWLa-zX"
      },
      "source": [
        "\n",
        "def model_report(model, dataloader):\n",
        "    # local final score on validation data\n",
        "    accuracy = evaluate_model(model, dataloader)\n",
        "    print(\"accuracy is \", accuracy)\n",
        "    return accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWfad1eUwXAB",
        "outputId": "dbe23745-5b41-4025-dc5e-caac0bce9e75"
      },
      "source": [
        "test_acc = model_report(model, cifar100_test_loader)\n",
        "print(test_acc)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is  0.7201\n",
            "0.7201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBL2XURfwXC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f1c374-fcaa-4b47-8e66-61d7331b5930"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8n7sq8ZyfBG"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWg7TJdgwXGx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Axid0GyA7w"
      },
      "source": [
        "## Model SVD Compression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kgOtOQHyEAq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "idx = [0, 3, 7, 10, 14, 17, 20, 24, 27, 30, 34, 37, 40]\n",
        "# idx = [40]\n",
        "conv_layers = ['features.'+str(i)+'.weight' for i in idx]\n",
        "new_model = vgg16_bn()   \n",
        "new_state_dict = model.state_dict()\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li15WD_j4_Gy",
        "outputId": "3bb1a374-d168-4b3c-d4e3-691c2afd91b2"
      },
      "source": [
        "\n",
        "# for sval_nums in range(5, 10):\n",
        "\n",
        "for i in range(1,4):\n",
        "    print(\"===========================================\")\n",
        "    retain = 0\n",
        "    original = 0\n",
        "    new_model = vgg16_bn()   \n",
        "    new_state_dict = model.state_dict()\n",
        "    for conv in conv_layers:\n",
        "        conv1_weight = model.state_dict()[conv].cpu().numpy()\n",
        "        # print(conv1_weight.shape)\n",
        "        sval_nums = int(min(conv1_weight.shape[0], conv1_weight.shape[1]*conv1_weight.shape[2]*conv1_weight.shape[3])/i)\n",
        "        original += conv1_weight.shape[0]*conv1_weight.shape[1]*conv1_weight.shape[2]*conv1_weight.shape[3]\n",
        "        U,Sigma,VT = np.linalg.svd(np.reshape(conv1_weight,(conv1_weight.shape[0],\\\n",
        "                                      conv1_weight.shape[1]*conv1_weight.shape[2]*conv1_weight.shape[3])))\n",
        "        con_restruct1 = (U[:,0:sval_nums]).dot(np.diag(Sigma[0:sval_nums])).dot(VT[0:sval_nums,:])\n",
        "        retain += sval_nums*conv1_weight.shape[0]+ \\\n",
        "        sval_nums**2 + sval_nums*conv1_weight.shape[2]*conv1_weight.shape[3]*conv1_weight.shape[1]\n",
        "        conv1_weight = np.reshape(con_restruct1,(conv1_weight.shape[0],conv1_weight.shape[1],\\\n",
        "                                      conv1_weight.shape[2],conv1_weight.shape[3]))\n",
        "        new_state_dict[conv] = torch.from_numpy(conv1_weight)\n",
        "      \n",
        "    new_model.load_state_dict(new_state_dict)\n",
        "    model.to(DEVICE)\n",
        "    test_acc = evaluate_model(new_model, cifar100_test_loader)\n",
        "    print(\"rank percentage:\", 1/i)\n",
        "    print(\"percentage:\", retain/original)\n",
        "    print(\"Accuracy is\", test_acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================================\n",
            "rank percentage: 1.0\n",
            "percentage: 1.2456842965660362\n",
            "Accuracy is 0.7201\n",
            "===========================================\n",
            "rank percentage: 0.5\n",
            "percentage: 0.5921276174565262\n",
            "Accuracy is 0.6617\n",
            "===========================================\n",
            "rank percentage: 0.3333333333333333\n",
            "percentage: 0.3862768706683895\n",
            "Accuracy is 0.4288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8HCeUlWt-M5",
        "outputId": "9935850c-ecd8-459d-b5b5-d18445e43118"
      },
      "source": [
        "\n",
        "# only compress large conv\n",
        "idx = [17, 20, 24, 27, 30, 34, 37, 40]\n",
        "conv_layers = ['features.'+str(i)+'.weight' for i in idx]\n",
        "\n",
        "for i in range(3,5):\n",
        "    print(\"===========================================\")\n",
        "    retain = 0\n",
        "    original = 0\n",
        "    new_model = vgg16_bn()   \n",
        "    new_state_dict = model.state_dict()\n",
        "    for conv in conv_layers:\n",
        "        conv1_weight = model.state_dict()[conv].cpu().numpy()\n",
        "        # print(conv1_weight.shape)\n",
        "        sval_nums = int(min(conv1_weight.shape[0], conv1_weight.shape[1]*conv1_weight.shape[2]*conv1_weight.shape[3])/i)\n",
        "        original += conv1_weight.shape[0]*conv1_weight.shape[1]*conv1_weight.shape[2]*conv1_weight.shape[3]\n",
        "        U,Sigma,VT = np.linalg.svd(np.reshape(conv1_weight,(conv1_weight.shape[0],\\\n",
        "                                      conv1_weight.shape[1]*conv1_weight.shape[2]*conv1_weight.shape[3])))\n",
        "        con_restruct1 = (U[:,0:sval_nums]).dot(np.diag(Sigma[0:sval_nums])).dot(VT[0:sval_nums,:])\n",
        "        retain += sval_nums*conv1_weight.shape[0]+ \\\n",
        "        sval_nums**2 + sval_nums*conv1_weight.shape[2]*conv1_weight.shape[3]*conv1_weight.shape[1]\n",
        "        conv1_weight = np.reshape(con_restruct1,(conv1_weight.shape[0],conv1_weight.shape[1],\\\n",
        "                                      conv1_weight.shape[2],conv1_weight.shape[3]))\n",
        "        new_state_dict[conv] = torch.from_numpy(conv1_weight)\n",
        "      \n",
        "    new_model.load_state_dict(new_state_dict)\n",
        "    new_model.to(DEVICE)\n",
        "    test_acc = evaluate_model(new_model, cifar100_test_loader)\n",
        "    print(\"rank percentage:\", 1/i)\n",
        "    print(\"percentage:\", retain/original)\n",
        "    print(\"Accuracy is\", test_acc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================================\n",
            "rank percentage: 0.3333333333333333\n",
            "percentage: 0.38526817604347513\n",
            "Accuracy is 0.6523\n",
            "===========================================\n",
            "rank percentage: 0.25\n",
            "percentage: 0.28761574074074076\n",
            "Accuracy is 0.5463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adj1qVn60wiN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}