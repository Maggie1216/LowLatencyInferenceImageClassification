{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from models import vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean and std of cifar100 dataset\n",
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "WARM = False # typically used in new training\n",
    "\n",
    "DATE_FORMAT = '%A_%d_%B_%Y_%Hh_%Mm_%Ss'\n",
    "#time of we run the script\n",
    "TIME_NOW = datetime.datetime.now().strftime(DATE_FORMAT)\n",
    "#data settings\n",
    "subset = False #for local running\n",
    "k = 10 #number of samples needed to each class in validation set, because we need to split train and validation\n",
    "\n",
    "#model settings\n",
    "USE_TENSORBOARD = False\n",
    "if USE_TENSORBOARD:\n",
    "    foo = SummaryWriter()\n",
    "use_gpu = True\n",
    "\n",
    "#lr scheduler\n",
    "BASE_LR = 0.001\n",
    "EPOCH_DECAY = 4\n",
    "DECAY_WEIGHT = 0.5\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files\n",
    "def unpickle(file):\n",
    "    \n",
    "    with open(file, 'rb') as fo:\n",
    "        dictionary = pickle.load(fo, encoding='bytes')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(cifar100_dataset):\n",
    "    \"\"\"compute the mean and std of cifar100 dataset\n",
    "    Args:\n",
    "        cifar100_training_dataset or cifar100_test_dataset\n",
    "        witch derived from class torch.utils.data\n",
    "\n",
    "    Returns:\n",
    "        a tuple contains mean, std value of entire dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data_r = numpy.dstack([cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))])\n",
    "    data_g = numpy.dstack([cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))])\n",
    "    data_b = numpy.dstack([cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))])\n",
    "    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n",
    "    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "def reshape_images(data_dict):\n",
    "    reshaped = data_dict.numpy().reshape(len(data_dict), 1024, 3, order = 'F').reshape(len(data_dict), 32,32,3)\n",
    "    reshaped_processed = torch.from_numpy(reshaped).float().permute(0, 3, 1, 2)\n",
    "    return reshaped_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_val_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 training dataset\n",
    "        std: std of cifar100 training dataset\n",
    "        path: path to cifar100 training python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle\n",
    "    Returns: train_data_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #cifar100_training = CIFAR100Train(path, transform=transform_train)\n",
    "    cifar100_training = torchvision.datasets.CIFAR100(root='', train=True, download=True, transform=transform_train)\n",
    "    \n",
    "    try:\n",
    "        random_index = pickle.load(open(\"random_index.pkl\", 'rb'))\n",
    "    except:\n",
    "        random_index = np.random.permutation([i for i in range(50000)])\n",
    "        pickle.dump(random_index, open(\"random_index.pkl\", 'wb'))\n",
    "    \n",
    "    train_index = random_index[:45000]\n",
    "    validation_index = random_index[45000:]\n",
    "    train_dataset = torch.utils.data.Subset(cifar100_training, train_index)\n",
    "    validation_dataset = torch.utils.data.Subset(cifar100_training, validation_index)\n",
    "    \n",
    "    cifar100_training_loader = DataLoader(\n",
    "        train_dataset, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "    \n",
    "    cifar100_validation_loader = DataLoader(\n",
    "        validation_dataset, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_training_loader, cifar100_validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 training dataset\n",
    "        std: std of cifar100 training dataset\n",
    "        path: path to cifar100 training python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle\n",
    "    Returns: train_data_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #cifar100_training = CIFAR100Train(path, transform=transform_train)\n",
    "    cifar100_training = torchvision.datasets.CIFAR100(root='', train=True, download=True, transform=transform_train)\n",
    "    cifar100_training_loader = DataLoader(\n",
    "        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_training_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 test dataset\n",
    "        std: std of cifar100 test dataset\n",
    "        path: path to cifar100 test python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle\n",
    "    Returns: cifar100_test_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #cifar100_test = CIFAR100Test(path, transform=transform_test)\n",
    "    cifar100_test = torchvision.datasets.CIFAR100(root='', train=False, download=True, transform=transform_test)\n",
    "    cifar100_test_loader = DataLoader(\n",
    "        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWeightCallBack():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.zeros_mask = None\n",
    "    \n",
    "    # on batch begin\n",
    "    def get_zeros_mask(self, model):\n",
    "        \n",
    "        self.zeros_mask= []\n",
    "\n",
    "        for weights_matrix in model.parameters():\n",
    "            self.zeros_mask.append(torch.where(weights_matrix == 0, \\\n",
    "                                     torch.zeros(weights_matrix.data.shape).to(DEVICE), \\\n",
    "                                     torch.ones(weights_matrix.data.shape).to(DEVICE)))\n",
    "    # on batch end\n",
    "    def apply_zeros_mask(self, model):\n",
    "        \n",
    "        for index, weights_matrix in enumerate(model.parameters()):\n",
    "            weights_matrix.data = weights_matrix.data * self.zeros_mask[index].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_dataloader, optimizer, loss_function, callbacks = None):\n",
    "\n",
    "    start = time.time()\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    # keep track of the zero mask\n",
    "    if callbacks != None:\n",
    "        callbacks.get_zeros_mask(model)\n",
    "    \n",
    "    for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "        '''\n",
    "        if epoch <= WARM:\n",
    "            warmup_scheduler.step()\n",
    "        '''\n",
    "            \n",
    "        if use_gpu:\n",
    "            labels = labels.to(DEVICE)\n",
    "            images = images.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if callbacks != None:\n",
    "            callbacks.apply_zeros_mask(model)\n",
    "            \n",
    "        n_iter = (epoch - 1) * len(train_dataloader) + batch_index + 1\n",
    "\n",
    "        last_layer = list(model.children())[-1]\n",
    "        for name, para in last_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
    "            if 'bias' in name:\n",
    "                writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n",
    "                \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "                loss.item(),\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                epoch=epoch,\n",
    "                trained_samples=batch_index * BATCH_SIZE + len(images),\n",
    "                total_samples=len(train_dataloader.dataset)\n",
    "            ))\n",
    "        \n",
    "\n",
    "        #update training loss for each iteration\n",
    "        writer.add_scalar('Train/loss', loss.item(), n_iter)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n",
    "\n",
    "    finish = time.time()\n",
    "\n",
    "    print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_dataloader):\n",
    "    # for validation set or testing set\n",
    "    start = time.time()\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    total_preds = 0\n",
    "    total_corrects = 0\n",
    "    \n",
    "    for batch_index, (images, labels) in enumerate(val_dataloader):\n",
    "        if use_gpu:\n",
    "            # labels = labels.to(DEVICE)\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        \n",
    "        total_preds += len(labels)\n",
    "        total_corrects += np.sum(preds.cpu().numpy() == labels.numpy())\n",
    "    \n",
    "    print(\"Accuracy is {:.5f}\".format(total_corrects/total_preds))\n",
    "    \n",
    "    return total_corrects/total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "cifar100_training_loader, cifar100_validation_loader = get_training_dataloader(\n",
    "    CIFAR100_TRAIN_MEAN,\n",
    "    CIFAR100_TRAIN_STD,\n",
    "    num_workers = 4,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "'''\n",
    "cifar100_training_loader = get_training_dataloader(\n",
    "    CIFAR100_TRAIN_MEAN,\n",
    "    CIFAR100_TRAIN_STD,\n",
    "    num_workers = 4,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    CIFAR100_TRAIN_MEAN,\n",
    "    CIFAR100_TRAIN_STD,\n",
    "    num_workers = 4,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg(path):\n",
    "    model = vgg.vgg16_bn()\n",
    "    weights = torch.load(path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using local port 19268\n",
      "INFO:tensorflow:Using local port 19410\n",
      "INFO:tensorflow:Using local port 23955\n",
      "INFO:tensorflow:Using local port 20259\n",
      "INFO:tensorflow:Using local port 20492\n",
      "INFO:tensorflow:Using local port 24257\n",
      "INFO:tensorflow:Using local port 16502\n",
      "INFO:tensorflow:Using local port 23239\n",
      "INFO:tensorflow:Using local port 15395\n",
      "INFO:tensorflow:Using local port 20986\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=os.path.join(\n",
    "            'logs', 'vgg', TIME_NOW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsgd_optimizer = optim.SGD(model.parameters(), lr= 0.0001, momentum=0.9, weight_decay=5e-4)\\ncrossEntropyLoss_function = nn.CrossEntropyLoss()\\ntrain(model, train_dataloader = cifar100_training_loader, epoch = 10, optimizer = sgd_optimizer,                                                           loss_function = crossEntropyLoss_function)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train block #\n",
    "'''\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr= 0.0001, momentum=0.9, weight_decay=5e-4)\n",
    "crossEntropyLoss_function = nn.CrossEntropyLoss()\n",
    "train(model, train_dataloader = cifar100_training_loader, epoch = 10, optimizer = sgd_optimizer, \\\n",
    "                                                          loss_function = crossEntropyLoss_function)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(model, dataloader):\n",
    "    # local final score on validation data\n",
    "    num_zeros = sum([(i.detach().cpu().numpy() == 0).sum() for i in model.parameters()])\n",
    "    total_parameters = sum([np.prod(i.shape) for i in model.parameters()])\n",
    "    accuracy = evaluate_model(model, dataloader)\n",
    "    result = (accuracy + num_zeros/total_parameters)/2\n",
    "    print(\"num_zeros / total_parameters ratio is \", num_zeros/total_parameters)\n",
    "    print(\"accuracy is \", accuracy)\n",
    "    print(\"overall score is \", result)\n",
    "    return num_zeros/total_parameters, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_network(model, threshold = 0.01):\n",
    "    \n",
    "    # vgg has classifier and features\n",
    "    for weights_matrix in model.parameters():\n",
    "        weights_matrix.data = torch.where(torch.abs(weights_matrix.data) >= threshold, \\\n",
    "                                          weights_matrix.data, torch.zeros(weights_matrix.data.shape).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_prune(model, rounds, epoches, train_dataloader, test_dataloader, lr = 0.00001, threshold = 0.001):\n",
    "    print(\"Model performance at the beginning...\")\n",
    "    model_report(model, test_dataloader)\n",
    "    sgd_optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9, weight_decay = 5e-4)\n",
    "    crossEntropyLoss_function = nn.CrossEntropyLoss()\n",
    "    prune_callback = ClipWeightCallBack()\n",
    "    \n",
    "    sparsity = []\n",
    "    accuracy = []\n",
    "    \n",
    "    print(\"Start pruning..\")\n",
    "    for i in range(rounds):\n",
    "        print(\"Round {}/{}:\".format(i + 1, rounds))\n",
    "        for epoch in range(1 + epoches):\n",
    "            prune_network(model)\n",
    "            train(model, epoch, train_dataloader, sgd_optimizer, crossEntropyLoss_function, callbacks = prune_callback)\n",
    "            zeros_percentage, test_acc = model_report(model, test_dataloader)\n",
    "        sparsity.append(zeros_percentage)\n",
    "        accuracy.append(test_acc)\n",
    "        pickle.dump([sparsity, accuracy], open(\"prune_hist/sparsity_accuracy.pkl\", \"wb\"))\n",
    "    \n",
    "    print(\"Done pruning.\")\n",
    "    \n",
    "    return sparsity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vgg(\"checkpoints/vgg72.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetune_prune(model, 10, 4, cifar100_training_loader, cifar100_test_loader, lr = 0.00001, threshold = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning threshold:\n",
    "\n",
    "Sparsity:[0.8850553143641191,\n",
    "  0.8854645702199087,\n",
    "  0.8858486316019958,\n",
    "  0.8862241968313407,\n",
    "  0.8865842396778212,\n",
    "  0.8869207049654809,\n",
    "  0.8872260078936021,\n",
    "  0.8874967382417068,\n",
    "  0.8877426562959903,\n",
    "  0.8879710528726462]\n",
    "  \n",
    "Accuracy: [0.7039,\n",
    "  0.7166,\n",
    "  0.7147,\n",
    "  0.7208,\n",
    "  0.7244,\n",
    "  0.7241,\n",
    "  0.7257,\n",
    "  0.7267,\n",
    "  0.7249,\n",
    "  0.7296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type VGG. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\1\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, open(\"checkpoints/vgg_sparsity8897_acc7296.pth\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_prune(model, 10, 4, cifar100_training_loader, cifar100_test_loader, lr = 0.00001, threshold = 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
