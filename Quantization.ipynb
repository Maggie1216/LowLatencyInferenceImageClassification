{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files\n",
    "def unpickle(file):\n",
    "    \n",
    "    with open(file, 'rb') as fo:\n",
    "        dictionary = pickle.load(fo, encoding='bytes')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "def reshape_images(data_dict):\n",
    "    reshaped = data_dict.numpy().reshape(len(data_dict), 1024, 3, order = 'F').reshape(len(data_dict), 32,32,3)\n",
    "    reshaped_processed = torch.from_numpy(reshaped).float().permute(0, 3, 1, 2)\n",
    "    return reshaped_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data settings\n",
    "subset = True #for local running\n",
    "k = 0 #number of samples needed to each class in validation set, because we need to split train and validation\n",
    "if k == 0:\n",
    "    ENABLE_VAL = False #we do not have validation set then\n",
    "#model settings\n",
    "USE_TENSORBOARD = False\n",
    "if USE_TENSORBOARD:\n",
    "    foo = SummaryWriter()\n",
    "use_gpu = False\n",
    "\n",
    "#lr scheduler\n",
    "BASE_LR = 0.001\n",
    "EPOCH_DECAY = 4\n",
    "DECAY_WEIGHT = 0.5\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = unpickle('../cifar-100-python/test')\n",
    "train_dict = unpickle('../cifar-100-python/train')\n",
    "meta = unpickle('../cifar-100-python/meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = meta[b'fine_label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subset:\n",
    "    train_data = torch.from_numpy(train_dict[b'data'][:100])\n",
    "    train_y = torch.tensor(train_dict[b'fine_labels'][:100])\n",
    "    test_data = torch.from_numpy(test_dict[b'data'][:100])\n",
    "    test_y = torch.tensor(test_dict[b'fine_labels'][:100])\n",
    "else:\n",
    "    train_data = torch.from_numpy(train_dict[b'data'])\n",
    "    train_y = torch.tensor(train_dict[b'fine_labels'])\n",
    "    test_data = torch.from_numpy(test_dict[b'data'])\n",
    "    test_y = torch.tensor(test_dict[b'fine_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_plot(phase, sample_id, test_y = test_y, label_names = label_names, test_data = test_data, train_y = train_y, train_data = train_data):\n",
    "    \n",
    "    if phase == 'train':\n",
    "        data = train_data\n",
    "        y = train_y\n",
    "    elif phase == 'test':\n",
    "        data = test_data\n",
    "        y = test_y\n",
    "    assert sample_id < len(data)\n",
    "    plt.imshow(data[sample_id].numpy().reshape(-1,3, order = 'F').reshape(32,32,3))\n",
    "    labeli = y[sample_id].item()\n",
    "    plt.title('label: ' + label_names[labeli].decode(\"utf-8\") + ', label id: ' + str(labeli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAl00lEQVR4nO2de5RldXXnP/s+6v2urq4u+k3TIA1CA81r8BVFRUyCzkRHjagTRpwkruiMySzirBUxMVmaGXU5yYxJExhQ8YkaWFGjhKWi0SAtNE03DfT7WV3V1fV+3teeP+5pKIrfPlXdXXWr4ezPWrWq6rfv75x9fvfse875fe/eP1FVHMd5+ZNabAccx6kMHuyOkxA82B0nIXiwO05C8GB3nITgwe44CSHRwS4i+0Xk+jm+VkXkvNPcz5z7ishPROQ/G7ZVIjIqIunT8WMO+75bRD41D9t5nYgcnuNrPyAiPz/N/Zh9ZxsrEbldRL5yOvt9qZLoYH+poaoHVbVBVYuL7cvZznyPlYi8U0R2isiIiDwlIm+bj+1WEg92x5kFEVkOfAX4b0AT8CfAV0Vk6aI6dop4sEeIyFUi8ksRGRSRbhH5WxGpmvGyG0Vkr4j0icj/FJHUtP6/F33yD4jID0Vk9Rm4s05EfiUiQyJyv4i0RftYEz0SZKL/20Tk/4nI0Wi//xi1bxeR35rmWzbyeWP0/6tE5BfRsR4SkQ8YY/KbIrI1et0vROSS0zkYEblNRPZMuyq+/cUvkb+JjvdpEXnDNEOziNwZvSdHRORTc3mMCYzVWhH5aeTDg8CSUziEFcCgqv5Ay3wPGAPWncI2Fh0P9ucpAv+V8klwLfAG4A9mvObtwCbgcuAm4PcAolu6jwP/HugAfgZ8LbQTEXmPiGybxZf3Rds+BygA/9t43ZeBOuAiYCnw+aj9S8B7p73uRqBbVbeKyCrgB8DfRL5uBLYG/LwcuAv4ENAO/D3wgIhUz+J7iD3Aq4Fm4JPAV0Ska5r9amAv5bH/BPCdkx9wwD2Ux+A84DLgTUBwTmMWvgr8OtrHXwDvn24UkW0i8h6j7xZgp4j8toiko/d7CpjtfTy7UNXE/gD7gesN20eB7077X4Ebpv3/B8BD0d8/AG6ZZksB48DqaX3Pm6NPPwE+Pe3/DUAOSANrom1lgC6gBLQGtnEOMAI0Rf/fB/z36O8/nX5cM/rdDXwq+vuLwF/MsD8DvHYOx/A64HCMfStwU/T3B4CjgEyz/wq4GeikHFS102zvBn48re/PjX1MH6tVlD8w6qfZvwp85RTOlVuA0Wg748BbF/v8PdUfv7JHiMj5IvJPInJMRIaBv+LFt3qHpv19gHJQAawGvhDd7g4C/YAAy0/TnZn7yQZ8WQn0q+rAzM6qehT4V+A/iEgL8Bbg3mn99szBh9XAx04eU3RcK3n+mOeMiLxv2uPAIHDxjOM5olFERZwc29WUj717Wt+/p3wXcyqcAwyo6tiMfczV/+uBv6b8IVYFvBb4h5OPRS8VPNif54vA08B6VW2ifFsuM16zctrfqyhfkaAcnB9S1ZZpP7Wq+ovT9GXmfvJA34zXHALaomAOcQ/lW/l3AL9U1SPT+s3lWfMQ8JczjqlOVYOPJxbR3MUdwIeBdlVtAbbzwrFdLiLT/z85tocoX9mXTPOhSVUvOhUfgG6gVUTqZ+xjrmwEHlbVLapaUtVHgUeAOcm2Zwse7M/TCAwDoyLyCuD3A6/5ExFpFZGVwEeAb0Ttfwf8qYhcBM9NKr3jDHx5r4hsEJE64M+B+3SGhKSq3ZQfH/5v5FNWRF4z7SX/SHlu4SOUn+FPci9wfSQlZUSk3bhC3QH8FxG5WsrUi8hbRaQxOsa7ReTuORxLPeXb6eNRv/9E+co+naXAH0XH8A7gQuD70TH+CPisiDSJSEpE1onIa+ew3+dQ1QOUn7s/KSJVIvIq4Ldm6TadR4FXT5vgvIzyHMRL6pndg/15/hh4D+Vn3Tt4PpCncz/lSZ6twPeAOwFU9bvAZ4CvR48A2ynfOr8IEfldEdkxiy9fpvz8fAyoAf7IeN3NlK/6TwO9lOcZiHyaAL4NrAW+M639IOUJu49RftzYClw6c8OqugX4IPC3wACwm/Iz8klWUn5UiEVVnwI+C/wS6AFeGej3CLCe8t3LXwK/o6onItv7KN86PxX5cR/l+YpT5T2UJwL7KU8CTv8ARER2iMjvGsfwU+B24D4RGaE8rn+lqj86DT8WDXnho5LzckJE/gw4X1XfO+uLT227VcATwCWqmp/PbTsLhwf7y5RIunocuFlVH15sf5zFx2/jX4aIyAcpT279wAPdOYlf2R0nIfiV3XESQqaSO0un05rNhndZKpXMfi+UYJ+ntqbG7NNQX2faqqvtb3wOjw6atrKC9GKKJfvuKJ+PSboKHxYANdVZ01Yo2nNi6ZQxvmqPbyFfMG2pjO1kVZXtY1V1ePxzuZj5vJLtR6loj7GkY+5OTfft45K4Nybma/latP23zh0ASYXHUcV+z1KGj8OD40yMTwWNZxTsInID8AXKX+X8B1X9dNzrs9kMK1aHv4A1OTFp9stkwm5ecuEFZp/rrrzctK1ZY3+n5Mf/dr9pKzEVbB8ctt/k3uNDpo2UfQKct95Wl/r7e01bU1N7sH1qaizYDtDbO/P7Os/T0GoH9PLVy0zbqnM3BduPHDoabAdgtN80jYyGxx4g02h/oKatM1ztm9p0yr6IpNMNpm1izB7HskIaJlvXGWwvpeyYqE2FL1hf2/yQ2ee0b+OjzKP/Q1lP3gC8W0Q2nO72HMdZWM7kmf0qYLeq7lXVHPB1yplgjuOchZxJsC/nhQkbhwkkfojIrSKyRUS2FIteYMVxFoszCfbQJMCLHkJVdbOqblLVTen0gpROcxxnDpxJsB/mhdlZK3g+C8xxnLOMM5mNfxRYLyJrgSPAuygnG5iUVMnlckGb1Q62LFcs2Y8FdXW1ca6YTE3ZM6ANrWG5IzMZM9NabcsnR7vt2dvGJnumPpOx75AO7D8SbG9onFlh63lyOVtNGB60Zahs9YhpGxgKZ/dm07bsWR3zBa/jPcdMW+2kfWwN7eHr2cS4/T4X8rYCkcYe+2Jh1LQ1tZ1r2hpqrwy2791nJ9V1tYWPq1Syr9+nHeyqWhCRDwM/pCy93aWqs2VzOY6zSJyRzq6q3we+P0++OI6zgPjXZR0nIXiwO05C8GB3nITgwe44CaGiWW8AZv58TKJRKhX+TBobtZM7xsbHT8Wt58hkbGno4P5w9eHJvJ2kITFJFR0draatULSlyGzWzugrFsJSX8+x4/b2MvWmLaf2KbJnt52Qk6kO+19XY++rubbJtHW02olBTz6137TVtIXPt9Y2+33p77eTl6rL9TaDdLSGk5AAWurs4z527GfB9nRMUNRkw2OViosj2+Q4zssJD3bHSQge7I6TEDzYHScheLA7TkKo7Gy8KqVSOGkklbITDKzZ+Jo6e+a8xqiBBlBt1MEDWNJqJ9AMD4R9LMXM4De3tZm21SvtdR9zuWHTtm+/nVxo1dcbjUn8iJvCnRyNKY0UM/5XXPrKYHt768pgO8C+PfZai//u6hctWvMc9Q32Oo+H+ncF21MZ+7jizo+O1hbT1tJoqwnZrJ2s03/86WB7vmD7eLgUnt3P5SfMPn5ld5yE4MHuOAnBg91xEoIHu+MkBA92x0kIHuyOkxAqKr0pSsFI1EiJLU1YSxdVVdnu19faiQdx0sq6cztM25L2sBSyY48thTW32vta2m7XOtu/z+433G/XvMtkwgkeDTUtZp9UlV2DbvWKFabtglecZ9qqasISpuZsP1653k4MamiypdTLr1lr2poOnwi2H9h32Oxz/jp7NaH2Bns8jh+3k436+rpN28RYWC7LxFRjHuwPJ3pZiVDgV3bHSQwe7I6TEDzYHScheLA7TkLwYHechODB7jgJocJZb0KpaEhssTXojOWfYpYtmpy0s396++xll7bt2GvahobDy/sc67XrxR3eb9dpe/QXh0xbKmvXOjt3rS3/XHvlJcH2UsGWNp/d94xpK2VsebCkdu29Z3eFl2s6uteuG7jpIvu41p5/hWkbOWHXG9RCWL5as8qWDVMxJ2NfT3h5LYC66gbTtuzcC01bS224dl0hb/tRXR1+P3c88j2zzxkFu4jsB0aAIlBQ1U1nsj3HcRaO+biy/4aq2pdKx3HOCvyZ3XESwpkGuwI/EpFfi8itoReIyK0iskVEtlhLLzuOs/Cc6W38dap6VESWAg+KyNOq+vD0F6jqZmAzQDabtRfgdhxnQTmjK7uqHo1+9wLfBa6aD6ccx5l/TvvKLiL1QEpVR6K/3wT8+ew9i8HWktoyg1WsLyO2+5PjYZkMoKWlxbQdPGov/fPs3v3B9raGZrNPNm0XsGxqsgslDoyEpSuA6jp7m1PaE2xP0WL26e2zs/YOxCwb1d5oS01XXrAh2H7tuovNPuMp+8YvFSMBHtkXzmwDONEflkXbO+zrXBF7PCZi6nY2Ndly3lVX2Jl0Rw+F/d+976DZZ2lXZ7C9uvpfzD5nchvfCXxXRE5u56uq+s9nsD3HcRaQ0w52Vd0L2PV9Hcc5q3DpzXESgge74yQED3bHSQge7I6TECqb9SZKJhuW3opGdhJAMR/uk5+0s66KBbsoY3WNXbywGJPxlKoJfwNww8V2tlZKbcmIdLg4JMAyO6GP4dF+05YzxnFiOGasYr7r1NhgnyLZAVvyqjn8ZLC9fo39Pu/O20VC6/vscbzw/HDWGMCR42Epddeu/WafzqVLTNvYlF04cs+Rnaata39YKgMYGwqfV919dlHMsUJ4LcBczn6f/cruOAnBg91xEoIHu+MkBA92x0kIHuyOkxAqOhsvCKlUeDY2X4rLfg3PVjY1xiSZ1LfYfqg91d25xJ4hH5toCraPT9pJN1VZO1nknOV2Ikxjk/3WTI22mLZqY3w1ZdfJe0VrzFJZKbtmXFzdwAYdCLZX5ezagJsuvtq01dgl+SjkbB9LPeFzpKc/PJsN0NmxxrRJylZy8thZMg/8yE4bWXnO+mB7+5JlZp+soeSkUvZ541d2x0kIHuyOkxA82B0nIXiwO05C8GB3nITgwe44CaGi0psChcKpF5i1etTW2ZJRe0eraaurtw+7pc1eJqllKJyMMTBiJ6asWW3Lay3NYSkPYOlS28emFXZSRd8+o3Zdv+1jTXHQtA3ZLtI9YienZAiP487d9vJJV11ky1rkbJ3v14/aCSh7DoTHY3LM3lf/cXtf6UybaSvKoGlbtdoeyKnJsHTY1HS+2aejZWWwvSpbbfbxK7vjJAQPdsdJCB7sjpMQPNgdJyF4sDtOQvBgd5yEUNkadApqLuQat8JrWOKRmCWBWpa0mLZMjV0HrabOTq8aGwtnbK1et9zss7zd9mN1gy3/LFF7GaqDTz1r2mQyPI6tq86xtzdmf+bnY5bYqhrqs/0gLAE902PXBnz63vtMW7pkZ8s1tdvvWVV1WEZbsdKuGzg0NGLaxsbGTVtjmy171S+1lwhLFcPSW0/PAbMPpXBdxkLBzm6c9couIneJSK+IbJ/W1iYiD4rIrui3LWo7jnNWMJfb+LuBG2a03QY8pKrrgYei/x3HOYuZNdij9dZnfv3qJuCe6O97gLfNr1uO48w3p/vM3qmq3QCq2i0i5ndCReRW4FaAVMrnAx1nsVjw6FPVzaq6SVU3ebA7zuJxutHXIyJdANHv3vlzyXGcheB0b+MfAN4PfDr6ff/cuxqfL2IXgcxkwhlUdfV21lv/YLjgIcDgiF0Y8PBBe/mcqqqucHvKLlKpA3aWl44PmjZa7W22L7/ItA3UhmWoI8fsz+Nsoy1hTk7ZWYpNzbaEeSIffj+Pj9lyXY0eN21veuMVpu2cteEMMICpfFiyGxy1JbRUzh778WE7I27Pvl2m7dBeW0pdvTos3WZiorO1Nfw+pzP29Xsu0tvXgF8CF4jIYRG5hXKQv1FEdgFvjP53HOcsZtYru6q+2zC9YZ59cRxnAfEZM8dJCB7sjpMQPNgdJyF4sDtOQqh4wcmimfZmyzhWRlxP3wmzx759dsbQiuVrTVu7IWkApLNhP1LjdsbesvpB01bdtNq0lTZeYtp6D+41bauWXhpsL6S6zT5PPP1vth89MRlgx+1MtB4j+ao6bct8ne12cc7qRjujbCpvr/U2Nhx2JBWTZZmbso8rn7f92LjRlgeP9hw0bX194bUCL7/sQrNPfX14HOO+uOZXdsdJCB7sjpMQPNgdJyF4sDtOQvBgd5yE4MHuOAmhsgUnAYykoZLarqSNj6SqrC3XrY4pKLhmjS15FauOmrZDPeFCj0e295h9+sN1AQGYaLKPuWPIzjbrPW6s5wa0NIczx3J2Mh8n9g2btvq8XcDwUJ8ty9UYMlpK7YzDkXHbtv+wnRF3TtHul9HacHuV/cYUM3Zm2+CwPfZrz7OLenZ2bTRtD/7Lg8H2ySlbUtSUIQGKLSn6ld1xEoIHu+MkBA92x0kIHuyOkxA82B0nIVR0Nl4EUtYEeszqT+3tTcH2dSs7zD7LltlJFfmCPdNdIw2mrd5Y+qejyZ7Z7cvbS0M1xigQS2PqoNVU2cd29Gh4trjvuF0D7Zx2+5hzo3ZtwJo62/9UOpxMYr7/QGOTXVNQYhKl7HcTxKjJdmJg5lIIz1NXa49HS7vtYyptnwei9jimi+GT/3iPnei17rzzwz7EDLBf2R0nIXiwO05C8GB3nITgwe44CcGD3XESgge74ySEiifCpA1lIJ2xJYOWlrD0li/Y2R1FtZMZyNnJHfUxGuCq+vZg+xKxZbJnBu2adlVpW+LpaF1m2ian7KWLDuwP1zob7bUTOJa02H7smrAlu9alrabt4JGng+0tLbZs2NAYTloBKORNEyNj4RpuAC3N4fdmeNROMinEyGRDg/b5cUWLXYNuVVenaTu6L5y01TdkJyhNTYUHREu2EDmX5Z/uEpFeEdk+re12ETkiIlujnxtn247jOIvLXG7j7wZuCLR/XlU3Rj/fn1+3HMeZb2YNdlV9GLC/buQ4zkuCM5mg+7CIbItu882HNxG5VUS2iMiWUinmO7GO4ywopxvsXwTWARuBbuCz1gtVdbOqblLVTXEF7B3HWVhOK/pUtUdVi6paAu4ArppftxzHmW9OS3oTkS5VPbme0NuB7XGvfx6lVApnBjU327KLGEXo9h6x5aRLB2OyvJpbTFupZMs4mY6wbLS8ts3sc+yA7WNmyh7+yTG7rlpDgy3jrF8Vlpp684Nmn/ZmOzNv+74dpq1pWUwm2qHw8kSFgi1rTU3YNe1K2NlmxX57rCbGwxLVaIxcVx+z1FQxJmPSOLUBELGP+5zlLcH2E8ODZp/u7r5gez5v72fWYBeRrwGvA5aIyGHgE8DrRGQj5ezC/cCHZtuO4ziLy6zBrqrvDjTfuQC+OI6zgPiMmeMkBA92x0kIHuyOkxA82B0nIVQ2600BQ55oba4zuw0Oh2WStNjuFwbC0gTAJHbW2FjeTq860Rde5qknRiKpabNluZYmO9tsdMjeZl/OlqiWDh8Otuf67eWTnjlmZwHm8nZmYX7Kfs8mJsLflqyrtqWrBonZV8GW+TJqj3FDTdjH1pixL8VkvbW3xMhyRVvOq6qz5dLOVV3B9v3H7HO473hYbiwUfPknx0k8HuyOkxA82B0nIXiwO05C8GB3nITgwe44CaGy0psIKUMuy6RtV8bHw5JMtsWWY/qOhAsvAkz1xqzJZdeHpKfvULD94HF7e8UeO1vrUNou2FiXsuWfVDEsAQJkOsL76560M8P6DJkMYKxgy3KlE4OmjVLY/4ZwMhwA119rF2zcecgulvTkbntNtNxE2I8lnbYjxZI9VpmYwqiHu58xbUPjB0zbxGRYshscGTT7NBrFT+NWvvMru+MkBA92x0kIHuyOkxA82B0nIXiwO05CqPjyT9bnS6lkL9dkzdSn1Z4F7+u3Z2insva+mrTKtC1PhWet6xvsGeueMTvpJle0kyq0zk7UWNNkJ1VMVofHap+RIAOQE3s2vpSyZ58nJiZMmzUpvCQm4akxbY9jFts2MNBr2iYmwjX56hrCyScApGL2NWTXNty+83HTlqmyZ8k7O8Mz6+PDMdJFR/gcLhZjVBx7a47jvJzwYHechODB7jgJwYPdcRKCB7vjJAQPdsdJCHNZEWYl8CVgGVACNqvqF0SkDfgGsIbyqjDvVNWB2I2poBr+fOlcakshQ4ODwfZ02pbeOtetNW0rV9i2/kE74eKpHeHkmrqULas0GlIYQLZliWlbd94rTNvQiaOm7Qljuabe4WHbj6wtN3Y0LzNt1Vn72KqM2m8TE7YUebxnn2kbHR8zbRdeusK0TY2HpajeHluuS2dt6U1T9jk3Nm5LmG21tpTaezz83oycsM+rTDo8voWi7d9cruwF4GOqeiFwDfCHIrIBuA14SFXXAw9F/zuOc5Yya7CrareqPhb9PQLsBJYDNwH3RC+7B3jbAvnoOM48cErP7CKyBrgMeAToPLmSa/Q7vMSp4zhnBXP+uqyINADfBj6qqsMi9ldOZ/S7FbgVIBXz1UvHcRaWOV3ZRSRLOdDvVdXvRM09ItIV2buA4IyHqm5W1U2quimV8sl/x1ksZo0+KV/C7wR2qurnppkeAN4f/f1+4P75d89xnPliLrfx1wE3A0+KyNao7ePAp4FvisgtwEHgHbNuSUCMj5d83pYMtBS2FYt2VlBLR4dpy6fsfo3tK03bigvCNdJy4/ZyTH0jtmQkMXX3BvL2NncfO2LaekbCMk5dTAJVKiZTqjlGMuoftJXW4ZGwLRvjyPaj9hJV2mz7UcjFLLtUFc4srG+wx77nuF3jr7nVXmoqm7Uz+go5+xG2ZGRaasy1OFMTlvninq5nDXZV/TlgbeINs/V3HOfswB+iHScheLA7TkLwYHechODB7jgJwYPdcRJChQtOlihqOKPo8GF7uab6mrBcU9Vir9UkMZ9jY4Y8BZBO15q2t17/5mD75KSd7XTv9x4wbfXVdsHJgWPdpu3qKy42bW9uuS7YXizamszAoD0eO3bYMt/YuL1M0nXXnh9sH+q3swrXX2Bnr+UILwEGMDxm20TCkldNrS0Bjo/bmX4jY7YkWlttj3GxmDdtdXXhJbsO5+zsu5qqsFyXStk++JXdcRKCB7vjJAQPdsdJCB7sjpMQPNgdJyF4sDtOQqiw9JZCJCw3DQ3a64Z1nBvONFpqrJEFMGgUqQQoqZ2B1FRvf/71PhuWw4YH7ayr5qwtr23YsM605absNcWWdtqZV1aWV3W2yezTGJNR9tjjz5i2mhjpsGt5WEYbOG5ntl18wQbTls3a71lBbBlNU+FTvFCwpbC4uguFkl0ws+/EMdOWjSnOeXB/uPjlvqd+afaZHAmPR6loy8B+ZXechODB7jgJwYPdcRKCB7vjJAQPdsdJCBWdjU+nhZbWmqAtN2EnM1RVh5fBqW+wly3au9deSmj9entppfo6e5u7tj8dbJ+KSSTZeKU9415UexZ/tDRo2gqD9oxwa2u4fH8+xsfcmO1HptpOdmlM2TXXJifDNejOWWmrAiMTdu23nkN9pq0oth+pVDixaWzMVn+mpuyZ+mLeHg/E3mZTk33cO54Mn6tN7XZ4Ll0ejqNs1r5++5XdcRKCB7vjJAQPdsdJCB7sjpMQPNgdJyF4sDtOQphVehORlcCXgGVACdisql8QkduBDwInMxs+rqrfj9tWW3s9//G9m4K2/KQtdwydCNct23fgWbPP8i5b8prK2/vqG7VrpO3uDUsk62KWmso02zXBsjE1y2om7ISGqqK9VFZhJFwjrSpj19Zr7rITYa685kLT9q8/t8e/c1m4PuDFF11g9pnM2fXdlqyz+5G25dKUcT1Lpew+kxP2cljDgydMW3t7s2l79JHtpm1/9+5g+zvf9xtmn9al4dD91ld/ZfaZi85eAD6mqo+JSCPwaxF5MLJ9XlX/1xy24TjOIjOXtd66ge7o7xER2QksX2jHHMeZX07pmV1E1gCXAY9ETR8WkW0icpeItM63c47jzB9zDnYRaQC+DXxUVYeBLwLrgI2Ur/yfNfrdKiJbRGTLWEx9b8dxFpY5BbuIZCkH+r2q+h0AVe1R1aKqloA7gKtCfVV1s6puUtVN9fV2ZRPHcRaWWYNdRAS4E9ipqp+b1t417WVvB+zpRsdxFp25zMZfB9wMPCkiW6O2jwPvFpGNgAL7gQ/NtqG6umquuPzcoG3whC1pFKeWBNt3NthLJE2M2zLO9r1PmLZU2pZdLrxgTbB93QW2PDVZjFkuKGb0O7vC2WsAmgpnAQLUNoSzodIZ+64qG2Nb32zLSX39dvbdo48+HmxfsdreXn1jTP20jL0vVVuKLBmbTGfsR8qmVlsSbWqz/X96h30+/uCHdj25V18fPn8uvcyeB88ay5RVG8tCwdxm438OhI4+VlN3HOfswr9B5zgJwYPdcRKCB7vjJAQPdsdJCB7sjpMQKlpwslRSJibCMslkTJG/xvpwVtZV18UUcyzZywVNxWTYLVlif+u3oSYsdwi29FOXtjPKBFvmy4j91uRKOdM2VQxvU0q2XJeKWZoolbVlqMuuXm3ajveHlzT61jcfNvtcfe1Fpm31OtvHukZbOlTC79n4pF04Mp+z389tj4cLaQL88w8fMm0XXNRl2t7y29cG21PpmHOgGD6/NSicRdszLY7jvKzwYHechODB7jgJwYPdcRKCB7vjJAQPdsdJCBWV3rJVabpWhrOGOrvC2VoAWgzLCTW19hpfBUOCApgYtzOeslk7a6h/6FiwvaE+Rq6rrTdtdXV2EUi13WdsyM6km5wKrzfW1t5m9hmftNeBU1uxQ1K2k696fXg9vV/8dK/Z52cPP2naBobtDLAlXbbcVFUVlj53PW0XFt322GHT1j9o97vmtWtN25tveLVpyxvnd1XWPnfSmbB8XM5ID+NXdsdJCB7sjpMQPNgdJyF4sDtOQvBgd5yE4MHuOAmhotKbpCBTE5YG8uO25NVYF5aN8sUxs0++YBcobGyyM9Gqs02mLWtkh8VJgJMxslauaGffpVNZ05apsbOypoz9DY7Z+6Jk7yudso9NxPaDTDgz7w2/ud7s0t9nZ6JlMvapOjRiZ6LliuFtNrTamuLl19gy36q1wYrpACzpss/hmCRGJsbD53FVlZ3Nl5saDbarVWETv7I7TmLwYHechODB7jgJwYPdcRKCB7vjJIRZZ+NFpAZ4GKiOXn+fqn5CRNYCXwfagMeAm1XVLo4GUBJKU+HaWSnsmcfh0fDSUEMTB80+hYKdpLG0eqVpy6ZtP2prw8k6w8PH7e3VxdR3U9tWjJm+TcUs8dPZGV4qqzprH9fQsD2rniuGE2sAaowkE7DHUbGTkFaus5dWyk3a7+fKVeGkG4Ce3qPB9rY2ezw6O+2lt4aGB03brj07TNu5a19p2qYKQ8H24oAdTsV8eDyKRfu9nMuVfQp4vapeSnl55htE5BrgM8DnVXU9MADcModtOY6zSMwa7FrmpKiXjX4UeD1wX9R+D/C2hXDQcZz5Ya7rs6ejFVx7gQeBPcCg6nNZ14cB+5sIjuMsOnMKdlUtqupGYAVwFRBaYzb4lSQRuVVEtojIlsEB+xtvjuMsLKc0G6+qg8BPgGuAFpHnZpFWAMGZEFXdrKqbVHVTS6tdecNxnIVl1mAXkQ4RaYn+rgWuB3YCPwZ+J3rZ+4H7F8hHx3HmgbkkwnQB94hImvKHwzdV9Z9E5Cng6yLyKeBx4M7ZNpRKpagzllCqytr12E70h+Wf5qawzFTel738U1XWrndXXWPLWuli+LMxo/YdS6HKHuL8qC2tdJ8I17sDqKqzZaPqdDjBI5u1P9ebxd7emNjJKamUnUwiEt5fPmcn5BQmY5aoUjtZZ3LClgerq8LbrCo2mn0mhmOSsmptefD8c+3lyKqr7QSrlpqwL0Mx9e5yOSMRJmYpslmDXVW3AZcF2vdSfn53HOclgH+DznESgge74yQED3bHSQge7I6TEDzYHSchiMat7zPfOxM5DhyI/l0C9FVs5zbuxwtxP17IS82P1araETJUNNhfsGORLaq6aVF27n64Hwn0w2/jHScheLA7TkJYzGDfvIj7no778ULcjxfysvFj0Z7ZHcepLH4b7zgJwYPdcRLCogS7iNwgIs+IyG4RuW0xfIj82C8iT4rIVhHZUsH93iUivSKyfVpbm4g8KCK7ot+ti+TH7SJyJBqTrSJyYwX8WCkiPxaRnSKyQ0Q+ErVXdExi/KjomIhIjYj8SkSeiPz4ZNS+VkQeicbjGyJi5+KGUNWK/gBpyjXszgWqgCeADZX2I/JlP7BkEfb7GuByYPu0tr8Gbov+vg34zCL5cTvwxxUejy7g8ujvRuBZYEOlxyTGj4qOCSBAQ/R3FniEcnWobwLvitr/Dvj9U9nuYlzZrwJ2q+peLdeZ/zpw0yL4sWio6sPAzMoEN1Gu0gsVqtZr+FFxVLVbVR+L/h6hXAlpORUekxg/KoqWmfeKzosR7MuBQ9P+X8zKtAr8SER+LSK3LpIPJ+lU1W4on3SAvVLBwvNhEdkW3eYv+OPEdERkDeViKY+wiGMyww+o8JgsREXnxQj20ALti6X/XaeqlwNvAf5QRF6zSH6cTXwRWEd5QZBu4LOV2rGINADfBj6qqvbC9pX3o+JjomdQ0dliMYL9MDB9/SWzMu1Co6pHo9+9wHdZ3DJbPSLSBRD97l0MJ1S1JzrRSsAdVGhMRCRLOcDuVdXvRM0VH5OQH4s1JtG+BznFis4WixHsjwLro5nFKuBdwAOVdkJE6kWk8eTfwJuA7fG9FpQHKFfphUWs1nsyuCLeTgXGRESEcsHSnar6uWmmio6J5Uelx2TBKjpXaoZxxmzjjZRnOvcA/2ORfDiXshLwBLCjkn4AX6N8O5infKdzC9AOPATsin63LZIfXwaeBLZRDrauCvjxKsq3pNuArdHPjZUekxg/KjomwCWUKzZvo/zB8mfTztlfAbuBbwHVp7Jd/7qs4yQE/wad4yQED3bHSQge7I6TEDzYHScheLA7TkLwYHechODB7jgJ4f8Dw5fdPyPROZoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see pictures by sample id\n",
    "see_plot('train', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleFromClass(ds, k): \n",
    "    #k: number of samples needed to each class in test set\n",
    "    class_counts = {}\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    for data, label in ds:\n",
    "        c = label.item()\n",
    "        class_counts[c] = class_counts.get(c, 0) + 1\n",
    "        if class_counts[c] > k:\n",
    "            train_data.append(data)\n",
    "            train_label.append(torch.unsqueeze(label, 0))\n",
    "        else:\n",
    "            test_data.append(data)\n",
    "            test_label.append(torch.unsqueeze(label, 0))\n",
    "    train_data = torch.stack(train_data)\n",
    "    train_label = torch.cat(train_label)\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.cat(test_label)\n",
    "\n",
    "    return ((train_data, train_label), \n",
    "        (test_data, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_VAL:\n",
    "    train_dataset, validation_dataset = sampleFromClass(list(zip(train_data, train_y)), k)\n",
    "else:\n",
    "    train_dataset = train_data, train_y\n",
    "test_dataset = test_data, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean for 3 channels :  tensor([125.3588, 122.7590, 110.8747])\n",
      "std  for 3 channels :  tensor([67.4012, 65.8902, 71.2648])\n"
     ]
    }
   ],
   "source": [
    "print(\"mean for 3 channels : \", reshape_images(train_dataset[0]).mean((0, 2, 3)))\n",
    "print(\"std  for 3 channels : \", reshape_images(train_dataset[0]).std((0, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms\n",
    "train_transform_image = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        # transforms.Normalize(mean=[85.5980, 85.5980, 85.5980], std=[68.1695, 65.3810, 70.4030])\n",
    "])\n",
    "\n",
    "if ENABLE_VAL:\n",
    "    val_transform_image = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            # transforms.Normalize(mean=[85.5980, 85.5980, 85.5980], std=[68.1695, 65.3810, 70.4030])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Y, transforms = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.X[index]\n",
    "        label = self.Y[index]\n",
    "        \n",
    "        if self.transforms != None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = CIFAR_Dataset(reshape_images(train_dataset[0]), train_dataset[1], transforms = train_transform_image)\n",
    "if ENABLE_VAL:\n",
    "    val_images = CIFAR_Dataset(reshape_images(validation_dataset[0]), validation_dataset[1], transforms =  val_transform_image)\n",
    "test_images = CIFAR_Dataset(reshape_images(test_dataset[0]), test_dataset[1], transforms =  val_transform_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_images, batch_size=16, shuffle=True)\n",
    "if ENABLE_VAL:\n",
    "    valloader = torch.utils.data.DataLoader(val_images, batch_size=16, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_images, batch_size=len(test_images), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loaders = {'train': trainloader, 'test': testloader}\n",
    "dset_sizes = {'train': len(trainloader.dataset), 'test': len(testloader.dataset)}\n",
    "if ENABLE_VAL:\n",
    "    dset_loaders = {'train': trainloader, 'val': valloader, 'test': testloader}\n",
    "    dset_sizes = {'train': len(trainloader.dataset), 'val': len(valloader.dataset), 'test': len(testloader.dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=5):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    if ENABLE_VAL:\n",
    "        phases = ['train', 'val']\n",
    "    else:\n",
    "        phases = ['train']\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for index, (inputs, labels) in enumerate(dset_loaders[phase]):\n",
    "                \n",
    "                inputs, labels = inputs.float().to(DEVICE), labels.long().to(DEVICE)\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss = Variable(loss, requires_grad = True)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if index % 500 == 0 and index > 0:\n",
    "                    print('{}/{} with loss {:.4f}'.format(index, dset_sizes['train']/16, running_loss/index))\n",
    "                \n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / float(dset_sizes[phase])\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if USE_TENSORBOARD:\n",
    "                    foo.add_scalar('epoch_loss',epoch_loss,epoch)\n",
    "                    foo.add_scalar('epoch_acc',epoch_acc,epoch)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    pickle.dump(best_model, open('best_model.pkl', 'wb'))\n",
    "                    print('new best accuracy = ',best_acc)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "    if USE_TENSORBOARD:\n",
    "        foo.close()\n",
    "    return best_model\n",
    "\n",
    "# This function changes the learning rate over the training model.\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "#test model\n",
    "def test_model(model):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in dset_loaders['test']:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        corrects = torch.sum(preds == labels.data) \n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss\n",
    "        running_corrects += corrects\n",
    "    accuracy = (running_corrects / float(dset_sizes['test'])).item()\n",
    "    loss = (running_loss / dset_sizes['test']).item()\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                'test', loss, accuracy))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting = True):\n",
    "    if feature_extracting:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting model, criterion and optimers\n",
    "vgg16 = models.vgg16(pretrained = True)\n",
    "#change target output features count into 100\n",
    "set_parameter_requires_grad(vgg16, feature_extracting = True)\n",
    "vgg16.classifier[-1] = nn.Linear(in_features=4096, out_features=100, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.3546 Acc: 0.0100\n",
      "Epoch 1/2\n",
      "----------\n",
      "train Loss: 0.2888 Acc: 0.1300\n",
      "Epoch 2/2\n",
      "----------\n",
      "train Loss: 0.2682 Acc: 0.1600\n",
      "Training complete in 2m 27s\n",
      "Best val Acc: 0.000000\n",
      "returning and looping back\n"
     ]
    }
   ],
   "source": [
    "train_model(vgg16, criterion, optimizer, exp_lr_scheduler, num_epochs = 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.0486 Acc: 0.0200\n",
      "0.048613712191581726 0.019999999552965164\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = test_model(vgg16)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize tensorboard -- a little buggy...\n",
    "if USE_TENSORBOARD:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant Dynamics with no retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.0486 Acc: 0.0200\n",
      "0.04860617220401764 0.019999999552965164\n"
     ]
    }
   ],
   "source": [
    "vgg16_quant1 = torch.quantization.quantize_dynamic(\n",
    "    vgg16,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
    "loss, accuracy = test_model(vgg16_quant1)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  baseline  \t Size (KB): 538686.817\n",
      "model:  dynamic quantization  \t Size (KB): 178846.257\n",
      "3.01 times smaller\n"
     ]
    }
   ],
   "source": [
    "# compare the sizes\n",
    "f=print_size_of_model(vgg16,\"baseline\")\n",
    "q=print_size_of_model(vgg16_quant1,\"dynamic quantization\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant Static with no retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_quant2 = copy.deepcopy(vgg16)\n",
    "#add layers\n",
    "set_parameter_requires_grad(vgg16_quant2, feature_extracting = True)\n",
    "vgg16_quant2.features = nn.Sequential(torch.quantization.QuantStub(), vgg16_quant2.features)\n",
    "vgg16_quant2.classifier = nn.Sequential(vgg16_quant2.classifier, torch.quantization.DeQuantStub())\n",
    "#set configs\n",
    "vgg16_quant2.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "vgg16_quant2 = torch.quantization.prepare(vgg16_quant2)\n",
    "# l = list(vgg16_quant2.named_modules())\n",
    "# vgg16_quant2 = torch.quantization.fuse_modules(vgg16_quant2, [['features.1.0', 'features.1.1']])\n",
    "vgg16_quant2 = torch.quantization.convert(vgg16_quant2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.0461 Acc: 0.0000\n",
      "0.04605165496468544 0.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = test_model(vgg16_quant2)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  baseline  \t Size (KB): 538686.817\n",
      "model:  static quantization  \t Size (KB): 134857.157\n",
      "3.99 times smaller\n"
     ]
    }
   ],
   "source": [
    "# compare the sizes\n",
    "f=print_size_of_model(vgg16,\"baseline\")\n",
    "q=print_size_of_model(vgg16_quant2,\"static quantization\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant Static with Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.1743 Acc: 0.4400\n",
      "Training complete in 0m 52s\n",
      "Best val Acc: 0.000000\n",
      "returning and looping back\n"
     ]
    }
   ],
   "source": [
    "vgg16_quant3 = copy.deepcopy(vgg16)\n",
    "#add layers\n",
    "vgg16_quant3.features = nn.Sequential(torch.quantization.QuantStub(), vgg16_quant3.features)\n",
    "vgg16_quant3.classifier = nn.Sequential(vgg16_quant3.classifier, torch.quantization.DeQuantStub())\n",
    "#set configs\n",
    "vgg16_quant3.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "vgg16_quant3 = torch.quantization.prepare_qat(vgg16_quant3)\n",
    "#retrain\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16_quant3.parameters(), lr=0.001)\n",
    "train_model(vgg16_quant3, criterion, optimizer, exp_lr_scheduler, num_epochs = 1);\n",
    "vgg16_quant3.eval()\n",
    "vgg16_quant3 = torch.quantization.convert(vgg16_quant3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.0485 Acc: 0.0200\n",
      "0.048465121537446976 0.019999999552965164\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = test_model(vgg16_quant3)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  baseline  \t Size (KB): 538686.817\n",
      "model:  static quantization with retrain  \t Size (KB): 134857.157\n",
      "3.99 times smaller\n"
     ]
    }
   ],
   "source": [
    "# compare the sizes\n",
    "f=print_size_of_model(vgg16,\"baseline\")\n",
    "q=print_size_of_model(vgg16_quant3,\"static quantization with retrain\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
